{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -qU transformers==4.48.3 datasets==3.2.0 optimum==1.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Custom Dataset \n",
    "model_path = \"/kaggle/input/qa-arasquad/transformers/default/2/finetuned-AraT5-QA/checkpoint-8125\"\n",
    "\n",
    "print(\"Files in model directory:\", os.listdir(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load tokenizer with custom special tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    use_fast=True  # Required for SentencePiece (spiece.model)\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_answer(formatted_text, max_input_len=1024, max_output_len=64):\n",
    "    \"\"\"\n",
    "    Generate an answer from a formatted input: \"<context> ... <question> ...\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize formatted input\n",
    "    inputs = tokenizer(\n",
    "        formatted_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_len\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_output_len,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode and strip \"<answer>\" if desired\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Optional: remove <answer> prefix if present\n",
    "    if decoded.startswith(\"<answer>\"):\n",
    "        decoded = decoded.replace(\"<answer>\", \"\").strip()\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "context = \"تم تولية محمد على حكم مصر بإرادة الشعب المصرى، رغم اعتراض الباب العالى العثمانى، فسعى نحو جعل نقطة الارتكاز له ولأبنائه من بعده فى مصر لا فى الآستانة، وقد نجح فى فترة وجيزة فى جعل ولايته (مصر) أكثر حضارة وتقدمًا من الدولة العثمانية صاحبة السيادة؛ فكان من الطبيعى أن تستقل عنها. ولكن محمد على أراد أن يحمى هذا الاستقلال ويحيطه بسياج من الحدود الطبيعية فى الشام شرقًا والسودان جنوبًا، وبدأت من هنا سياسة (محمد على) الخارجية وتحركاته التوسعية لحماية الحدود المصرىة.\"\n",
    "\n",
    "question = \"من الذي تولى حكم مصر بإرادة الشعب المصري؟\"\n",
    "formatted_input = f\"<context>{context}<question>{question}\"\n",
    "\n",
    "answer = generate_answer(formatted_input)\n",
    "print(\"Generated Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_json(\"/kaggle/input/arabic-squad-v20/asquadv2-test.json\", encoding='ISO-8859-1')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normalize and clean data\n",
    "def normalize_data(data):\n",
    "    data = pd.json_normalize(data['data'], ['paragraphs', 'qas'], ['title', ['paragraphs', 'context']])\n",
    "    data = data.rename(columns={'paragraphs.context': 'context', 'question': 'question', 'answers': 'answers'})\n",
    "    data['answers'] = data['answers'].apply(lambda x: {'text': [x[0]['text']] if x else [], 'answer_start': [x[0]['answer_start']] if x else []})\n",
    "    data = data[data[\"is_impossible\"] == False].drop(['is_impossible', 'plausible_answers'], axis=1, errors='ignore')\n",
    "    return data\n",
    "\n",
    "test_data = normalize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract relevant fields\n",
    "def extract_relevant_data(dataset):\n",
    "    return pd.DataFrame({\n",
    "        \"context\": dataset[\"context\"],\n",
    "        \"question\": dataset[\"question\"],\n",
    "        \"answer\": dataset[\"answers\"].apply(lambda ans: ans[\"text\"][0] if ans[\"text\"] else \"\")\n",
    "    })\n",
    "\n",
    "\n",
    "test_data = extract_relevant_data(test_data)\n",
    "\n",
    "# Check lengths\n",
    "\n",
    "print(f\"Test data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "import pandas as pd\n",
    "def format_for_answer_generation(dataset):\n",
    "    return pd.DataFrame({\n",
    "        \"text\": \"<context>\" + dataset[\"context\"] + \"<question>\" + dataset[\"question\"],\n",
    "        \"required\": \"<answer>\" + dataset[\"answer\"]\n",
    "    })\n",
    "\n",
    "# # Process test data\n",
    "test_ag = format_for_answer_generation(test_data)  \n",
    "test_ag_dataset = Dataset.from_pandas(test_ag)\n",
    "\n",
    "\n",
    "# Create dataset dictionary including train, validation, and test sets\n",
    "datasets_ag = DatasetDict({\n",
    "    \"test\": test_ag_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datasets_ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datasets_ag['test'] = datasets_ag['test'].remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed= 42\n",
    "# datasets_ag[\"test\"] = datasets_ag[\"test\"].shuffle(seed=seed).select(range(min(1000, len(datasets_ag[\"test\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datasets_ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for split in datasets_ag:\n",
    "    print(f\"{split} columns: {datasets_ag[split].column_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datasets_ag[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_answers(context, questions, num_answers_per_question=1):\n",
    "    \"\"\"\n",
    "    Generate multiple answers for a list of questions based on shared context.\n",
    "    \n",
    "    Args:\n",
    "        context (str): Shared context.\n",
    "        questions (list): List of questions.\n",
    "        num_answers_per_question (int): Number of diverse answers to generate.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {question: [answer1, answer2, ...]}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    results = {}\n",
    "\n",
    "    for question in questions:\n",
    "        formatted_text = f\"<context>{context}<question>{question}\"\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            formatted_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=768\n",
    "        ).to(device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_answers_per_question\n",
    "        )\n",
    "\n",
    "        decoded = [\n",
    "            tokenizer.decode(output, skip_special_tokens=True).strip().replace(\"<answer>\", \"\").strip()\n",
    "            for output in outputs\n",
    "        ]\n",
    "\n",
    "        results[question] = decoded\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_answer_generation_f1(dataset, model, tokenizer, num_samples=20, csv_path=\"f1_evaluation_results.csv\"):\n",
    "    \"\"\"\n",
    "    Evaluates the model using token-level F1 with Arabic-aware tokenization and saves results to CSV.\n",
    "\n",
    "    Args:\n",
    "        dataset: List of dicts with \"text\" and \"required\" fields.\n",
    "        model: The fine-tuned QA model.\n",
    "        tokenizer: The tokenizer used.\n",
    "        num_samples: Number of samples to evaluate.\n",
    "        csv_path: Path to save the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (list of detailed scores, average F1 score)\n",
    "    \"\"\"\n",
    "    def arabic_tokenize(text):\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text.split()\n",
    "\n",
    "    results = []\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    \n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "\n",
    "        full_text = example[\"text\"]\n",
    "        reference_answer = example[\"required\"].replace(\"<answer>\", \"\").strip()\n",
    "\n",
    "        # Parse \"<context> ... <question> ...\"\n",
    "        try:\n",
    "            context_split = full_text.split(\"<context>\", 1)[1]\n",
    "            context, question = context_split.split(\"<question>\", 1)\n",
    "            context = context.strip()\n",
    "            question = question.strip()\n",
    "        except Exception:\n",
    "            print(f\"❌ Skipping sample {i+1} - invalid format\")\n",
    "            continue\n",
    "\n",
    "        generated_dict = generate_answers(context, [question], num_answers_per_question=1)\n",
    "        generated_answer = generated_dict[question][0].replace(\"<answer>\", \"\").strip()\n",
    "\n",
    "        ref_tokens = set(arabic_tokenize(reference_answer))\n",
    "        gen_tokens = set(arabic_tokenize(generated_answer))\n",
    "\n",
    "        tp = len(ref_tokens & gen_tokens)\n",
    "        precision = tp / len(gen_tokens) if gen_tokens else 0\n",
    "        recall = tp / len(ref_tokens) if ref_tokens else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        f1_scores.append(f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        \n",
    "        results.append({\n",
    "            \"Sample\": i + 1,\n",
    "            \"F1_Score\": f1,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"Context\": context,  # ✅ Now context is included\n",
    "            \"Question\": question,\n",
    "            \"Reference_Answer\": reference_answer,\n",
    "            \"Generated_Answer\": generated_answer\n",
    "        })\n",
    "\n",
    "    avg_f1 = np.mean(f1_scores) if f1_scores else 0.0\n",
    "    avg_per = np.mean(precision_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n📊 Evaluation Complete — Average F1: {avg_f1:.4f} from {len(f1_scores)} samples\")\n",
    "    print(f\"\\n📊 Evaluation Complete — Average per: {avg_per:.4f} from {len(f1_scores)} samples\")\n",
    "    print(f\"\\n📊 Evaluation Complete — Average recall: {avg_recall:.4f} from {len(f1_scores)} samples\")\n",
    "    print(f\"📁 Results saved to: {csv_path}\")\n",
    "    return results, avg_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "test_dataset = datasets_ag[\"test\"].shuffle(seed=seed).select(range(min(4000, len(datasets_ag[\"test\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results, final_score = evaluate_answer_generation_f1(test_dataset,\n",
    "                                                     model,\n",
    "                                                     tokenizer,\n",
    "                                                     len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_text(prediction) == normalize_text(ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6828613,
     "sourceId": 10973974,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 372329,
     "modelInstanceId": 351077,
     "sourceId": 431764,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
