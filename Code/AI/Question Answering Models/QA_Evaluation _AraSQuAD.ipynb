{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -qU transformers==4.48.3 datasets==3.2.0 optimum==1.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Custom Dataset \n",
    "model_path = \"/kaggle/input/qa-arasquad/transformers/default/2/finetuned-AraT5-QA/checkpoint-8125\"\n",
    "\n",
    "print(\"Files in model directory:\", os.listdir(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load tokenizer with custom special tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    use_fast=True  # Required for SentencePiece (spiece.model)\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_answer(formatted_text, max_input_len=1024, max_output_len=64):\n",
    "    \"\"\"\n",
    "    Generate an answer from a formatted input: \"<context> ... <question> ...\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize formatted input\n",
    "    inputs = tokenizer(\n",
    "        formatted_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_len\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_output_len,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode and strip \"<answer>\" if desired\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Optional: remove <answer> prefix if present\n",
    "    if decoded.startswith(\"<answer>\"):\n",
    "        decoded = decoded.replace(\"<answer>\", \"\").strip()\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "context = \"ØªÙ… ØªÙˆÙ„ÙŠØ© Ù…Ø­Ù…Ø¯ Ø¹Ù„Ù‰ Ø­ÙƒÙ… Ù…ØµØ± Ø¨Ø¥Ø±Ø§Ø¯Ø© Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù…ØµØ±Ù‰ØŒ Ø±ØºÙ… Ø§Ø¹ØªØ±Ø§Ø¶ Ø§Ù„Ø¨Ø§Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù‰ Ø§Ù„Ø¹Ø«Ù…Ø§Ù†Ù‰ØŒ ÙØ³Ø¹Ù‰ Ù†Ø­Ùˆ Ø¬Ø¹Ù„ Ù†Ù‚Ø·Ø© Ø§Ù„Ø§Ø±ØªÙƒØ§Ø² Ù„Ù‡ ÙˆÙ„Ø£Ø¨Ù†Ø§Ø¦Ù‡ Ù…Ù† Ø¨Ø¹Ø¯Ù‡ ÙÙ‰ Ù…ØµØ± Ù„Ø§ ÙÙ‰ Ø§Ù„Ø¢Ø³ØªØ§Ù†Ø©ØŒ ÙˆÙ‚Ø¯ Ù†Ø¬Ø­ ÙÙ‰ ÙØªØ±Ø© ÙˆØ¬ÙŠØ²Ø© ÙÙ‰ Ø¬Ø¹Ù„ ÙˆÙ„Ø§ÙŠØªÙ‡ (Ù…ØµØ±) Ø£ÙƒØ«Ø± Ø­Ø¶Ø§Ø±Ø© ÙˆØªÙ‚Ø¯Ù…Ù‹Ø§ Ù…Ù† Ø§Ù„Ø¯ÙˆÙ„Ø© Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠØ© ØµØ§Ø­Ø¨Ø© Ø§Ù„Ø³ÙŠØ§Ø¯Ø©Ø› ÙÙƒØ§Ù† Ù…Ù† Ø§Ù„Ø·Ø¨ÙŠØ¹Ù‰ Ø£Ù† ØªØ³ØªÙ‚Ù„ Ø¹Ù†Ù‡Ø§. ÙˆÙ„ÙƒÙ† Ù…Ø­Ù…Ø¯ Ø¹Ù„Ù‰ Ø£Ø±Ø§Ø¯ Ø£Ù† ÙŠØ­Ù…Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙ‚Ù„Ø§Ù„ ÙˆÙŠØ­ÙŠØ·Ù‡ Ø¨Ø³ÙŠØ§Ø¬ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙÙ‰ Ø§Ù„Ø´Ø§Ù… Ø´Ø±Ù‚Ù‹Ø§ ÙˆØ§Ù„Ø³ÙˆØ¯Ø§Ù† Ø¬Ù†ÙˆØ¨Ù‹Ø§ØŒ ÙˆØ¨Ø¯Ø£Øª Ù…Ù† Ù‡Ù†Ø§ Ø³ÙŠØ§Ø³Ø© (Ù…Ø­Ù…Ø¯ Ø¹Ù„Ù‰) Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© ÙˆØªØ­Ø±ÙƒØ§ØªÙ‡ Ø§Ù„ØªÙˆØ³Ø¹ÙŠØ© Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…ØµØ±Ù‰Ø©.\"\n",
    "\n",
    "question = \"Ù…Ù† Ø§Ù„Ø°ÙŠ ØªÙˆÙ„Ù‰ Ø­ÙƒÙ… Ù…ØµØ± Ø¨Ø¥Ø±Ø§Ø¯Ø© Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù…ØµØ±ÙŠØŸ\"\n",
    "formatted_input = f\"<context>{context}<question>{question}\"\n",
    "\n",
    "answer = generate_answer(formatted_input)\n",
    "print(\"Generated Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_json(\"/kaggle/input/arabic-squad-v20/asquadv2-test.json\", encoding='ISO-8859-1')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normalize and clean data\n",
    "def normalize_data(data):\n",
    "    data = pd.json_normalize(data['data'], ['paragraphs', 'qas'], ['title', ['paragraphs', 'context']])\n",
    "    data = data.rename(columns={'paragraphs.context': 'context', 'question': 'question', 'answers': 'answers'})\n",
    "    data['answers'] = data['answers'].apply(lambda x: {'text': [x[0]['text']] if x else [], 'answer_start': [x[0]['answer_start']] if x else []})\n",
    "    data = data[data[\"is_impossible\"] == False].drop(['is_impossible', 'plausible_answers'], axis=1, errors='ignore')\n",
    "    return data\n",
    "\n",
    "test_data = normalize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract relevant fields\n",
    "def extract_relevant_data(dataset):\n",
    "    return pd.DataFrame({\n",
    "        \"context\": dataset[\"context\"],\n",
    "        \"question\": dataset[\"question\"],\n",
    "        \"answer\": dataset[\"answers\"].apply(lambda ans: ans[\"text\"][0] if ans[\"text\"] else \"\")\n",
    "    })\n",
    "\n",
    "\n",
    "test_data = extract_relevant_data(test_data)\n",
    "\n",
    "# Check lengths\n",
    "\n",
    "print(f\"Test data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "import pandas as pd\n",
    "def format_for_answer_generation(dataset):\n",
    "    return pd.DataFrame({\n",
    "        \"text\": \"<context>\" + dataset[\"context\"] + \"<question>\" + dataset[\"question\"],\n",
    "        \"required\": \"<answer>\" + dataset[\"answer\"]\n",
    "    })\n",
    "\n",
    "# # Process test data\n",
    "test_ag = format_for_answer_generation(test_data)  \n",
    "test_ag_dataset = Dataset.from_pandas(test_ag)\n",
    "\n",
    "\n",
    "# Create dataset dictionary including train, validation, and test sets\n",
    "datasets_ag = DatasetDict({\n",
    "    \"test\": test_ag_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datasets_ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datasets_ag['test'] = datasets_ag['test'].remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed= 42\n",
    "# datasets_ag[\"test\"] = datasets_ag[\"test\"].shuffle(seed=seed).select(range(min(1000, len(datasets_ag[\"test\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datasets_ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for split in datasets_ag:\n",
    "    print(f\"{split} columns: {datasets_ag[split].column_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datasets_ag[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_answers(context, questions, num_answers_per_question=1):\n",
    "    \"\"\"\n",
    "    Generate multiple answers for a list of questions based on shared context.\n",
    "    \n",
    "    Args:\n",
    "        context (str): Shared context.\n",
    "        questions (list): List of questions.\n",
    "        num_answers_per_question (int): Number of diverse answers to generate.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {question: [answer1, answer2, ...]}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    results = {}\n",
    "\n",
    "    for question in questions:\n",
    "        formatted_text = f\"<context>{context}<question>{question}\"\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            formatted_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=768\n",
    "        ).to(device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_answers_per_question\n",
    "        )\n",
    "\n",
    "        decoded = [\n",
    "            tokenizer.decode(output, skip_special_tokens=True).strip().replace(\"<answer>\", \"\").strip()\n",
    "            for output in outputs\n",
    "        ]\n",
    "\n",
    "        results[question] = decoded\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_answer_generation_f1(dataset, model, tokenizer, num_samples=20, csv_path=\"f1_evaluation_results.csv\"):\n",
    "    \"\"\"\n",
    "    Evaluates the model using token-level F1 with Arabic-aware tokenization and saves results to CSV.\n",
    "\n",
    "    Args:\n",
    "        dataset: List of dicts with \"text\" and \"required\" fields.\n",
    "        model: The fine-tuned QA model.\n",
    "        tokenizer: The tokenizer used.\n",
    "        num_samples: Number of samples to evaluate.\n",
    "        csv_path: Path to save the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (list of detailed scores, average F1 score)\n",
    "    \"\"\"\n",
    "    def arabic_tokenize(text):\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text.split()\n",
    "\n",
    "    results = []\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    \n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "\n",
    "        full_text = example[\"text\"]\n",
    "        reference_answer = example[\"required\"].replace(\"<answer>\", \"\").strip()\n",
    "\n",
    "        # Parse \"<context> ... <question> ...\"\n",
    "        try:\n",
    "            context_split = full_text.split(\"<context>\", 1)[1]\n",
    "            context, question = context_split.split(\"<question>\", 1)\n",
    "            context = context.strip()\n",
    "            question = question.strip()\n",
    "        except Exception:\n",
    "            print(f\"âŒ Skipping sample {i+1} - invalid format\")\n",
    "            continue\n",
    "\n",
    "        generated_dict = generate_answers(context, [question], num_answers_per_question=1)\n",
    "        generated_answer = generated_dict[question][0].replace(\"<answer>\", \"\").strip()\n",
    "\n",
    "        ref_tokens = set(arabic_tokenize(reference_answer))\n",
    "        gen_tokens = set(arabic_tokenize(generated_answer))\n",
    "\n",
    "        tp = len(ref_tokens & gen_tokens)\n",
    "        precision = tp / len(gen_tokens) if gen_tokens else 0\n",
    "        recall = tp / len(ref_tokens) if ref_tokens else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        f1_scores.append(f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        \n",
    "        results.append({\n",
    "            \"Sample\": i + 1,\n",
    "            \"F1_Score\": f1,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"Context\": context,  # âœ… Now context is included\n",
    "            \"Question\": question,\n",
    "            \"Reference_Answer\": reference_answer,\n",
    "            \"Generated_Answer\": generated_answer\n",
    "        })\n",
    "\n",
    "    avg_f1 = np.mean(f1_scores) if f1_scores else 0.0\n",
    "    avg_per = np.mean(precision_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\nğŸ“Š Evaluation Complete â€” Average F1: {avg_f1:.4f} from {len(f1_scores)} samples\")\n",
    "    print(f\"\\nğŸ“Š Evaluation Complete â€” Average per: {avg_per:.4f} from {len(f1_scores)} samples\")\n",
    "    print(f\"\\nğŸ“Š Evaluation Complete â€” Average recall: {avg_recall:.4f} from {len(f1_scores)} samples\")\n",
    "    print(f\"ğŸ“ Results saved to: {csv_path}\")\n",
    "    return results, avg_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "test_dataset = datasets_ag[\"test\"].shuffle(seed=seed).select(range(min(4000, len(datasets_ag[\"test\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results, final_score = evaluate_answer_generation_f1(test_dataset,\n",
    "                                                     model,\n",
    "                                                     tokenizer,\n",
    "                                                     len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_text(prediction) == normalize_text(ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6828613,
     "sourceId": 10973974,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 372329,
     "modelInstanceId": 351077,
     "sourceId": 431764,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
