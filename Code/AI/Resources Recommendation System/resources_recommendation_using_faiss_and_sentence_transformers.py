# -*- coding: utf-8 -*-
"""Resource Recommendation Using FAISS and Sentence Transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1knaNJ-LQ1GBD5_PB7_NtSR5twnbZ6OFl

# **FAISS**

#  ***Loading Data***
"""

!git clone https://github.com/YoustinaEhab/Arabic-Dataset.git

import json

with open('Arabic-Dataset/all_data.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

paragraphs = [entry['paragraph'] for entry in data if 'paragraph' in entry]
metadata = [{'topic': entry['topic']} for entry in data if 'topic' in entry]

print(f"Loaded {len(paragraphs)} paragraphs!")
print(paragraphs[:2])

# Create paragraph IDs
paragraphs_id = [str(i) for i in range(len(paragraphs))]

"""# ***Text to Vector***"""

!pip install numpy<2.0 faiss-gpu-cu11 --force-reinstall

#!pip install faiss-cpu==1.11.0

# !pip uninstall numpy -y
# !pip install numpy --upgrade --force-reinstall

# !pip install --upgrade --force-reinstall transformers faiss-cpu

import json
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import numpy as np
import faiss
from copy import deepcopy
import pickle
import os

def average_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    last_hidden = last_hidden_states.masked_fill(~attention_mask.unsqueeze(-1).bool(), 0.0)
    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load tokenizer and model
model_id = 'intfloat/multilingual-e5-large-instruct'
#model_id = 'intfloat/multilingual-e5-large'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModel.from_pretrained(model_id).to(device)
model.eval()

import time
start_time = time.time()

batch_size = 16
encoded_paragraphs = []

for i in range(0, len(paragraphs), batch_size):
    batch = paragraphs[i:i+batch_size]
    batch_encoding = tokenizer(batch, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)
    with torch.no_grad():
        outputs = model(**batch_encoding)
        embeddings = average_pool(outputs.last_hidden_state, batch_encoding['attention_mask'])
        embeddings = F.normalize(embeddings, p=2, dim=1)
    encoded_paragraphs.append(embeddings.cpu().numpy())

encoded_paragraphs = np.vstack(encoded_paragraphs).astype(np.float32)

embedding_time = time.time() - start_time
print(f"Embedding Time: {embedding_time:.2f} seconds")

faiss.normalize_L2(encoded_paragraphs)

dim = encoded_paragraphs.shape[1]

start_time = time.time()

# Create FAISS index (Inner Product = cosine similarity for normalized vectors)
faiss_index = faiss.IndexIDMap(faiss.IndexFlatIP(dim))
faiss_index.add_with_ids(encoded_paragraphs, paragraphs_id)

print(f"FAISS index built with {faiss_index.ntotal} vectors.")
indexing_time = time.time() - start_time
print(f"FAISS Index Creation Time: {indexing_time:.2f} seconds")

subject_interests = ["الأدب العربي", "التاريخ", "علوم", "الرياضيات"]
question_domain = ['Applying', 'Analyzing', 'Understanding', 'Creating', 'Remembering', 'Evaluating']
difficulty_levels = ["سهل", "متوسط", "صعب"]

# Example interests
selected_interests = ["التاريخ", "متوسط", "Remembering"]

start_time = time.time()

query_string = " ".join(selected_interests)

query_encoding = tokenizer([query_string], max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)
with torch.no_grad():
    query_outputs = model(**query_encoding)
    query_embedding = average_pool(query_outputs.last_hidden_state, query_encoding['attention_mask'])
    query_embedding = F.normalize(query_embedding, p=2, dim=1)
query_embedding_np = query_embedding.cpu().numpy().astype(np.float32)

query_time = time.time() - start_time
print(f"ChromaDB Query Time: {query_time:.2f} seconds")

# Normalize query embedding (for consistency)
faiss.normalize_L2(query_embedding_np)

# Search top 5 most similar paragraphs
results = faiss_index.search(query_embedding_np, 5)

import os
import pickle

start_time = time.time()

# ✅ Ensure the directory exists
os.makedirs('/content/faiss-History_docs', exist_ok=True)

# ✅ Save FAISS index and metadata
with open('/content/faiss-History_docs/index.pickle', 'wb') as handle:
    pickle.dump(faiss_index, handle, protocol=pickle.HIGHEST_PROTOCOL)

with open('/content/faiss-History_docs/data.pickle', 'wb') as handle:
    pickle.dump(
        {"data": paragraphs, "ids": paragraphs_id, "metadata": metadata},
        handle,
        protocol=pickle.HIGHEST_PROTOCOL
    )

saving_time = time.time() - start_time
print(f"FAISS Index & Metadata Saving Time: {saving_time:.2f} seconds")


#Load
import pickle

with open('/content/faiss-History_docs/index.pickle','rb') as handle:
  loaded_faiss_index = pickle.load(handle)

with open('/content/faiss-History_docs/data.pickle','rb') as handle:
  loaded_faiss_data = pickle.load(handle)

loaded_faiss_data.keys()

total_time = embedding_time + indexing_time + query_time + saving_time
print("\n--- Total Pipeline Timing ---")
print(f"Total Time: {total_time:.2f} seconds")
print(f"  Embedding: {embedding_time:.2f}s")
print(f"  Insertion: {indexing_time:.2f}s")
print(f"  Saving: {saving_time:.2f}s")
print(f"  Query:     {query_time:.2f}s")

distances, indices = results
id_to_data = dict(zip(loaded_faiss_data["ids"], zip(loaded_faiss_data["data"], loaded_faiss_data["metadata"])))

print("Top matching paragraphs and their similarity scores:")
for i, doc_id in enumerate(indices[0]):
    str_doc_id = str(doc_id)  # Ensure ID is a string for lookup
    if str_doc_id in id_to_data:
        paragraph, meta = id_to_data[str_doc_id]
        similarity = distances[0][i]
        print(f"\nParagraph ID: {str_doc_id}")
        print(f"Similarity Score: {similarity:.4f}")
        print(f"Paragraph: {paragraph}")
        print(f"Metadata: {meta}")
    else:
        print(f"Warning: Could not find ID {str_doc_id} in loaded data.")